{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent with Minibatch and Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MomentumGradientDescent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate=0.001,\n",
    "        momentum=0.9,\n",
    "        max_iters=1e4,\n",
    "        epsilon=1e-8,\n",
    "        batch_size=10,\n",
    "        record_history=False,\n",
    "    ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.max_iters = max_iters\n",
    "        self.record_history = record_history\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.prev_delta_w = None\n",
    "        if record_history:\n",
    "            # to store the weight history for visualization\n",
    "            self.w_history = []\n",
    "\n",
    "    def run(self, gradient_fn, x, y, w):\n",
    "        grad = np.inf\n",
    "        t = 1\n",
    "        N, D = x.shape\n",
    "        self.prev_delta_w = np.zeros(w.shape)\n",
    "        while np.linalg.norm(grad) > self.epsilon and t < self.max_iters:\n",
    "            grad = gradient_fn(x, y, w)\n",
    "            delta_w = self.get_delta_w(grad)\n",
    "\n",
    "            # weight update step\n",
    "            w = w - self.learning_rate * delta_w\n",
    "            if self.record_history:\n",
    "                self.w_history.append(w)\n",
    "            t += 1\n",
    "        return w\n",
    "\n",
    "#     def run(self, gradient_fn, x, y, w):\n",
    "#         grad = np.inf\n",
    "#         t = 1\n",
    "#         N, D = x.shape\n",
    "#         self.prev_delta_w = np.zeros(D)\n",
    "#         while np.linalg.norm(grad) > self.epsilon and t < self.max_iters:\n",
    "#             for i in range(0, N, self.batch_size):\n",
    "#                 if x.ndim == 1:\n",
    "#                     batch_x = x[i : i + self.batch_size]\n",
    "#                 else:\n",
    "#                     batch_x = x[i : i + self.batch_size, :]\n",
    "\n",
    "#                 if y.ndim == 1:\n",
    "#                     batch_y = y[i : i + self.batch_size]\n",
    "#                 else:\n",
    "#                     batch_y = y[i : i + self.batch_size, :]\n",
    "\n",
    "#                 # compute the gradient with present weight\n",
    "#                 grad = gradient_fn(batch_x, batch_y, w)\n",
    "#                 delta_w = self.get_delta_w(grad)\n",
    "\n",
    "#                 # weight update step\n",
    "#                 w = w - self.learning_rate * delta_w\n",
    "#                 if self.record_history:\n",
    "#                     self.w_history.append(w)\n",
    "#             t += 1\n",
    "#         return w\n",
    "\n",
    "    def get_delta_w(self, grad):\n",
    "        beta = self.momentum\n",
    "        delta_w = beta * self.prev_delta_w + (1 - beta) * grad\n",
    "        self.prev_delta_w = delta_w\n",
    "\n",
    "        return delta_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# from the given Colab code\n",
    "logistic = lambda z: 1./ (1 + np.exp(-z))  \n",
    "\n",
    "class LinearRegression:\n",
    "\n",
    "    def __init__(self, add_bias=True):\n",
    "        self.add_bias = add_bias\n",
    "        pass\n",
    "            \n",
    "    def fit(self, x, y, C, optimizer):\n",
    "        if x.ndim == 1:\n",
    "            x = x[:, None]\n",
    "        if self.add_bias:\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "        N,D = x.shape\n",
    "        \n",
    "        def to_onehot(a):\n",
    "            return np.eye(C)[a]\n",
    "        \n",
    "        def gradient(x, y, w):\n",
    "            N, D = x.shape\n",
    "            # yh: N x C\n",
    "            yh = self.softmax(np.dot(x, w))\n",
    "            # both are N x C\n",
    "            yh = to_onehot(self.to_classlabel(yh))\n",
    "            y = to_onehot(y)\n",
    "            \n",
    "            grad = np.dot(x.T, yh - y) / N\n",
    "            return grad\n",
    "        \n",
    "        # initialize all weights to 0\n",
    "        w0 = np.zeros((D,C)) \n",
    "        # run the optimizer to get the optimal weights\n",
    "        self.w = optimizer.run(gradient, x, y, w0) \n",
    "        return self\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        # to prevent overflow/underflow\n",
    "        z = z - np.max(z, axis=-1, keepdims=True)\n",
    "        return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n",
    "\n",
    "    def to_classlabel(self, z):\n",
    "        return z.argmax(axis=1)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if self.add_bias:\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "        # convert from 1D to 2D\n",
    "        x = np.reshape(x, (1, -1))\n",
    "        yh = self.softmax(np.dot(x, self.w))\n",
    "        return self.to_classlabel(yh)[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of Model\n",
    "\n",
    "Validation is performed using K-fold cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd \n",
    "\n",
    "# Returns 2 datasets (training and validation)\n",
    "def k_fold_splitter(fold, dataset):\n",
    "    start = math.floor(fold*(dataset.shape[0]/5))\n",
    "    end = math.floor((fold+1)*(dataset.shape[0]/5))\n",
    "\n",
    "    training = np.delete(dataset, slice(start, end), axis=0)\n",
    "    validation = dataset[start:end-1]\n",
    "\n",
    "    return training, validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9146278458162804\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "x, y = digits.data, digits.target\n",
    "\n",
    "C = 10\n",
    "accuracies = []\n",
    "# do 5-fold cross-validation\n",
    "for fold_num in range(5):\n",
    "    train_data, validation_data = k_fold_splitter(fold_num, x)\n",
    "    train_labels, validation_labels = k_fold_splitter(fold_num, y)\n",
    "    \n",
    "    optimizer = MomentumGradientDescent(learning_rate=.005, max_iters=1000)\n",
    "    model = LinearRegression(add_bias=False)\n",
    "    model.fit(train_data, train_labels, C, optimizer)\n",
    "    \n",
    "    num_misclassified = 0\n",
    "    # calculate the accuracy\n",
    "    for i in range(len(validation_data)):\n",
    "        prediction = model.predict(validation_data[i, :])\n",
    "        if prediction != validation_labels[i]:\n",
    "            num_misclassified += 1\n",
    "            \n",
    "    misclassification_rate = num_misclassified / len(validation_labels)\n",
    "    accuracies.append(1 - misclassification_rate)\n",
    "    \n",
    "print(\"Accuracy: {}\".format(np.average(accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.924705882352941\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "x, y = wine.data, wine.target\n",
    "\n",
    "C = 3\n",
    "accuracies = []\n",
    "# do 5-fold cross-validation\n",
    "for fold_num in range(5):\n",
    "    train_data, validation_data = k_fold_splitter(fold_num, x)\n",
    "    train_labels, validation_labels = k_fold_splitter(fold_num, y)\n",
    "    \n",
    "    optimizer = MomentumGradientDescent(learning_rate=.005, max_iters=10000)\n",
    "    model = LinearRegression(add_bias=False)\n",
    "    model.fit(train_data, train_labels, C, optimizer)\n",
    "    \n",
    "    num_misclassified = 0\n",
    "    # calculate the accuracy\n",
    "    for i in range(len(validation_data)):\n",
    "        prediction = model.predict(validation_data[i, :])\n",
    "        if prediction != validation_labels[i]:\n",
    "            num_misclassified += 1\n",
    "            \n",
    "    misclassification_rate = num_misclassified / len(validation_labels)\n",
    "    accuracies.append(1 - misclassification_rate)\n",
    "    \n",
    "print(\"Accuracy: {}\".format(np.average(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
