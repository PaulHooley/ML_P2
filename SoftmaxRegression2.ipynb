{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent with Minibatch and Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MomentumGradientDescent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate=0.001,\n",
    "        momentum=0.9,\n",
    "        max_iters=1e4,\n",
    "        epsilon=1e-8,\n",
    "        batch_size=32,\n",
    "        record_history=False,\n",
    "    ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.max_iters = max_iters\n",
    "        self.record_history = record_history\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.prev_delta_w = None\n",
    "        if record_history:\n",
    "            # to store the weight history for visualization\n",
    "            self.w_history = []\n",
    "\n",
    "    def run(self, gradient_fn, x, y, w):\n",
    "        grad = np.inf\n",
    "        t = 1\n",
    "        N, D = x.shape\n",
    "        self.prev_delta_w = np.zeros(w.shape)\n",
    "        while np.linalg.norm(grad) > self.epsilon and t < self.max_iters:\n",
    "            for i in range(0, N, self.batch_size):\n",
    "                if x.ndim == 1:\n",
    "                    batch_x = x[i:i + self.batch_size]\n",
    "                else:\n",
    "                    batch_x = x[i:i + self.batch_size, :]\n",
    "\n",
    "                if y.ndim == 1:\n",
    "                    batch_y = y[i:i + self.batch_size]\n",
    "                else:\n",
    "                    batch_y = y[i:i + self.batch_size, :]\n",
    "\n",
    "                # compute the gradient with present weight\n",
    "                grad = gradient_fn(batch_x, batch_y, w)\n",
    "                delta_w = self.get_delta_w(grad)\n",
    "\n",
    "                # weight update step\n",
    "                w = w - self.learning_rate * delta_w\n",
    "                if self.record_history:\n",
    "                    self.w_history.append(w)\n",
    "            t += 1\n",
    "        return w\n",
    "\n",
    "    def get_delta_w(self, grad):\n",
    "        beta = self.momentum\n",
    "        delta_w = beta * self.prev_delta_w + (1 - beta) * grad\n",
    "        self.prev_delta_w = delta_w\n",
    "\n",
    "        return delta_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression\n",
    "\n",
    "Below is our implementation of the Softmax Regression model. Class labels and predictions are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# from the given Colab code\n",
    "logistic = lambda z: 1./ (1 + np.exp(-z))  \n",
    "\n",
    "class SoftmaxRegression:\n",
    "\n",
    "    def __init__(self, add_bias=True, regularization_penalty=0.):\n",
    "        self.add_bias = add_bias\n",
    "        self.regularization_penalty = regularization_penalty\n",
    "            \n",
    "    def fit(self, x, y, C, optimizer):\n",
    "        if x.ndim == 1:\n",
    "            x = x[:, None]\n",
    "        if self.add_bias:\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "        N,D = x.shape\n",
    "        \n",
    "        def to_onehot(a):\n",
    "            return np.eye(C)[a]\n",
    "        \n",
    "        def gradient(x, y, w):\n",
    "            N, D = x.shape\n",
    "            # yh: N x C\n",
    "            yh = self.softmax(np.dot(x, w))\n",
    "            # both are N x C\n",
    "            yh = to_onehot(self.to_classlabel(yh))\n",
    "            y = to_onehot(y)\n",
    "            \n",
    "            grad = np.dot(x.T, yh - y) / N\n",
    "            if self.regularization_penalty > 0:\n",
    "                if self.add_bias:\n",
    "                    grad[:-1,:] += self.regularization_penalty * w[:-1,:]    # don't penalize the intercept\n",
    "                else:\n",
    "                    grad += self.regularization_penalty * w\n",
    "            return grad\n",
    "        \n",
    "        # initialize all weights to random values\n",
    "        w0 = np.random.rand(D,C)  \n",
    "        # run the optimizer to get the optimal weights\n",
    "        self.w = optimizer.run(gradient, x, y, w0) \n",
    "        return self\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        # to prevent overflow/underflow\n",
    "        z = z - np.max(z, axis=-1, keepdims=True)\n",
    "        return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n",
    "\n",
    "    def to_classlabel(self, z):\n",
    "        return z.argmax(axis=1)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if self.add_bias:\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "        # convert from 1D to 2D\n",
    "        x = np.reshape(x, (1, -1))\n",
    "        yh = self.softmax(np.dot(x, self.w))\n",
    "        return self.to_classlabel(yh)[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of Model\n",
    "\n",
    "Throughout the code below, validation is performed using 5-fold cross-validation. Here we define some helper methods which are used in the following sections below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def k_fold_splitter(fold, dataset):\n",
    "    \"\"\"\n",
    "    Returns 2 datasets (training and validation)\n",
    "    \"\"\"\n",
    "    start = math.floor(fold*(dataset.shape[0]/5))\n",
    "    end = math.floor((fold+1)*(dataset.shape[0]/5))\n",
    "\n",
    "    training = np.delete(dataset, slice(start, end-1), axis=0)\n",
    "    validation = dataset[start:end-1]\n",
    "\n",
    "    return training, validation\n",
    "\n",
    "def calculate_model_accuracy(x, y, C, learning_rate, momentum, batch_size, regularization_penalty):\n",
    "    \"\"\"\n",
    "    Helper method to calculate the accuracy of our implemented Softmax\n",
    "    Regression model for a given set of inputs, labels, and hyper-parameters\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "    # do 5-fold cross-validation\n",
    "    for fold_num in range(5):\n",
    "        train_data, validation_data = k_fold_splitter(fold_num, x)\n",
    "        train_labels, validation_labels = k_fold_splitter(fold_num, y)\n",
    "\n",
    "        optimizer = MomentumGradientDescent(\n",
    "            learning_rate=learning_rate, \n",
    "            momentum=momentum, \n",
    "            batch_size=batch_size, \n",
    "            max_iters=10000,\n",
    "        )\n",
    "        model = SoftmaxRegression(add_bias=False, regularization_penalty=regularization_penalty)\n",
    "        model.fit(train_data, train_labels, C, optimizer)\n",
    "\n",
    "        num_misclassified = 0\n",
    "        # calculate the accuracy\n",
    "        for i in range(len(validation_data)):\n",
    "            prediction = model.predict(validation_data[i, :])\n",
    "            if prediction != validation_labels[i]:\n",
    "                num_misclassified += 1\n",
    "\n",
    "        misclassification_rate = num_misclassified / len(validation_labels)\n",
    "        accuracies.append(1 - misclassification_rate)\n",
    "        \n",
    "    return np.average(accuracies)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digits Dataset\n",
    "\n",
    "First we do a grid search to find a good set of hyper-parameters (learning rate, momentum, batch size). This will give us a reference point for the model's performance which we can compare to as we change individual hyper-parameters. These parameters are not optimal since our grid search does not try an extensive set of combinations, but they are at least a good starting point. We kept the number of combinations on the smaller side since each loop iteration is fairly expensive in terms of computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9168531457649275, 0.9174149172904251, 0.9202128818412412, "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-5d996cc62a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_model_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_accuracy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-5ca1cd7a3933>\u001b[0m in \u001b[0;36mcalculate_model_accuracy\u001b[0;34m(x, y, C, learning_rate, momentum, batch_size, regularization_penalty)\u001b[0m\n\u001b[1;32m     31\u001b[0m         )\n\u001b[1;32m     32\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSoftmaxRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularization_penalty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mnum_misclassified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-534c2824270a>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, C, optimizer)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mw0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# run the optimizer to get the optimal weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-a2fba7917a02>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, gradient_fn, x, y, w)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# compute the gradient with present weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0mdelta_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_delta_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-534c2824270a>\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(x, y, w)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_onehot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myh\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularization_penalty\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from itertools import product\n",
    "\n",
    "digits = load_digits()\n",
    "x, y = digits.data, digits.target\n",
    "\n",
    "C = 10\n",
    "max_accuracy = 0\n",
    "best_learning_rate = 0\n",
    "best_momentum = 0\n",
    "best_batch_size = x.shape[0]\n",
    "best_penalty = 0\n",
    "\n",
    "for penalty in np.arange(0, 1.05, 0.05):\n",
    "    accuracy = calculate_model_accuracy(x, y, C, best_learning_rate, best_momentum, x.shape[0], penalty)\n",
    "    if accuracy > max_accuracy:\n",
    "        print(accuracy, end=', ')\n",
    "        max_accuracy = accuracy\n",
    "        best_penalty = penalty\n",
    "print('done')\n",
    "\n",
    "# grid search to find good hyper-parameters\n",
    "# for learning_rate in np.linspace(0.0001, 0.2, 21):\n",
    "#     for momentum in np.linspace(0.5, 0.99, 11):\n",
    "for params in product(np.linspace(0.0001, 0.2, 21), np.linspace(0.5, 0.99, 11)):\n",
    "    learning_rate, momentum = params\n",
    "    accuracy = calculate_model_accuracy(x, y, C, learning_rate, momentum, x.shape[0], 0)\n",
    "    if accuracy > max_accuracy:\n",
    "        print(accuracy, end = ', ')\n",
    "        max_accuracy = accuracy\n",
    "        best_learning_rate = learning_rate\n",
    "        best_momentum = momentum\n",
    "        # best_penalty = penalty\n",
    "                      \n",
    "for batch_size in range(1, x.shape[0], 64):\n",
    "    accuracy = calculate_model_accuracy(x, y, C, best_learning_rate, best_momentum, batch_size)\n",
    "    if accuracy > max_accuracy:\n",
    "        print(accuracy, end = ' [BATCH], ')\n",
    "        max_accuracy = accuracy\n",
    "        best_batch_size = batch_size\n",
    "            \n",
    "print(\"\\n\\nBest Accuracy: {}\".format(max_accuracy))  \n",
    "print(\"Best Learning Rate: {}\".format(best_learning_rate))  \n",
    "print(\"Best Momentum: {}\".format(best_momentum))  \n",
    "print(\"Best Batch Size: {}\".format(best_batch_size))  \n",
    "print(\"Best Regularization penalty: {}\".format(best_penalty))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Dataset\n",
    "\n",
    "Similar to above, we first do a grid search to find a good set of hyper-parameters as a reference point for the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda\n",
      "0.33714285714285713, 0.6211764705882352, 0.7366386554621849, 0.8833613445378152, 0.924033613445378, "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "x, y = wine.data, wine.target\n",
    "\n",
    "C = 3\n",
    "max_accuracy = 0\n",
    "best_learning_rate = 0\n",
    "best_momentum = 0\n",
    "best_batch_size = x.shape[0]\n",
    "best_penalty = 0\n",
    "\n",
    "print('lambda')\n",
    "for penalty in np.arange(0, 1.05, 0.05):\n",
    "    accuracy = calculate_model_accuracy(x, y, C, best_learning_rate, best_momentum, x.shape[0], penalty)\n",
    "    if accuracy > max_accuracy:\n",
    "        print(accuracy, end=', ')\n",
    "        max_accuracy = accuracy\n",
    "        best_penalty = penalty           \n",
    "print('done')\n",
    "        \n",
    "# grid search to find good hyper-parameters\n",
    "for learning_rate in np.linspace(0.0001, 0.2, 21):\n",
    "    for momentum in np.linspace(0.5, 0.99, 11):\n",
    "        accuracy = calculate_model_accuracy(x, y, C, learning_rate, momentum, x.shape[0], 0)\n",
    "        if accuracy > max_accuracy:\n",
    "            print(accuracy, end=', ')\n",
    "            max_accuracy = accuracy\n",
    "            best_learning_rate = learning_rate\n",
    "            best_momentum = momentum\n",
    "            \n",
    "# use an increment of 16 instead of 64 since this dataset is much smaller\n",
    "for batch_size in range(1, x.shape[0], 16):\n",
    "    accuracy = calculate_model_accuracy(x, y, C, best_learning_rate, best_momentum, batch_size)\n",
    "    if accuracy > max_accuracy:\n",
    "        print(accuracy, end=' [BATCH], ')\n",
    "        max_accuracy = accuracy\n",
    "        best_batch_size = batch_size\n",
    "            \n",
    "print(\"\\n\\nBest Accuracy: {}\".format(max_accuracy))\n",
    "print(\"Best Learning Rate: {}\".format(best_learning_rate))\n",
    "print(\"Best Momentum: {}\".format(best_momentum))\n",
    "print(\"Best Batch Size: {}\".format(best_batch_size))\n",
    "print(\"Best Regularization penalty: {}\".format(best_penalty))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Against Other Classifiers\n",
    "\n",
    "Now we can compare our model to an off-the-shelf classifier to see how the performance compares on the digits dataset. We will be comparing against the Ridge Regression and Logistic Regression models from SciKit Learn.\n",
    "\n",
    "### Digits Dataset Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Regression Accuracy: 0.9146340704315215\n",
      "Ridge Regression (SKLearn) Accuracy: 0.8889653133315697\n",
      "Logistic Regression (SKLearn) Accuracy: 0.918524454957128\n",
      "Naive Bayes (SKLearn) Accuracy: 0.8716826691150154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "digits = load_digits()\n",
    "x, y = digits.data, digits.target\n",
    "\n",
    "C = 10\n",
    "# using the hyper-parameters obtained from grid search\n",
    "softmax_accuracy = calculate_model_accuracy(x, y, C, 0.0001, 0.549, 321)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_model_accuracies = []\n",
    "for fold_num in range(5):\n",
    "    train_data, validation_data = k_fold_splitter(fold_num, x)\n",
    "    train_labels, validation_labels = k_fold_splitter(fold_num, y)\n",
    "\n",
    "    ridge_model = RidgeClassifier(max_iter=10000)\n",
    "    ridge_model.fit(train_data, train_labels)\n",
    "\n",
    "    ridge_model_accuracies.append(ridge_model.score(validation_data, validation_labels))\n",
    "    \n",
    "# Logistic Regression\n",
    "logistic_model_accuracies = []\n",
    "for fold_num in range(5):\n",
    "    train_data, validation_data = k_fold_splitter(fold_num, x)\n",
    "    train_labels, validation_labels = k_fold_splitter(fold_num, y)\n",
    "\n",
    "    logistic_model = LogisticRegression(multi_class=\"multinomial\", max_iter=10000)\n",
    "    logistic_model.fit(train_data, train_labels)\n",
    "\n",
    "    logistic_model_accuracies.append(logistic_model.score(validation_data, validation_labels))\n",
    "    \n",
    "# Multinomial Naive Bayes\n",
    "nb_model_accuracies = []\n",
    "for fold_num in range(5):\n",
    "    train_data, validation_data = k_fold_splitter(fold_num, x)\n",
    "    train_labels, validation_labels = k_fold_splitter(fold_num, y)\n",
    "\n",
    "    nb_model = MultinomialNB()\n",
    "    nb_model.fit(train_data, train_labels)\n",
    "\n",
    "    nb_model_accuracies.append(nb_model.score(validation_data, validation_labels))\n",
    "\n",
    "print(\"Softmax Regression Accuracy: {}\".format(softmax_accuracy))\n",
    "print(\"Ridge Regression (SKLearn) Accuracy: {}\".format(np.average(ridge_model_accuracies)))\n",
    "print(\"Logistic Regression (SKLearn) Accuracy: {}\".format(np.average(logistic_model_accuracies)))\n",
    "print(\"Naive Bayes (SKLearn) Accuracy: {}\".format(np.average(nb_model_accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine Dataset Comparison\n",
    "\n",
    "We can do the same comparison on the wine dataset. This is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Regression Accuracy: 0.8843697478991597\n",
      "Ridge Regression (SKLearn) Accuracy: 0.9421848739495797\n",
      "Logistic Regression (SKLearn) Accuracy: 0.9359663865546219\n",
      "Gaussian Naive Bayes (SKLearn) Accuracy: 0.9594957983193277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "wine = load_wine()\n",
    "x, y = wine.data, wine.target\n",
    "\n",
    "C = 3\n",
    "# using the hyper-parameters obtained from grid search\n",
    "softmax_accuracy = calculate_model_accuracy(x, y, C, 0.0001, 0.892, 178)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_model_accuracies = []\n",
    "for fold_num in range(5):\n",
    "    train_data, validation_data = k_fold_splitter(fold_num, x)\n",
    "    train_labels, validation_labels = k_fold_splitter(fold_num, y)\n",
    "\n",
    "    ridge_model = RidgeClassifier(max_iter=10000)\n",
    "    ridge_model.fit(train_data, train_labels)\n",
    "\n",
    "    ridge_model_accuracies.append(ridge_model.score(validation_data, validation_labels))\n",
    "    \n",
    "# Logistic Regression\n",
    "logistic_model_accuracies = []\n",
    "for fold_num in range(5):\n",
    "    train_data, validation_data = k_fold_splitter(fold_num, x)\n",
    "    train_labels, validation_labels = k_fold_splitter(fold_num, y)\n",
    "\n",
    "    logistic_model = LogisticRegression(multi_class=\"multinomial\", max_iter=10000)\n",
    "    logistic_model.fit(train_data, train_labels)\n",
    "\n",
    "    logistic_model_accuracies.append(logistic_model.score(validation_data, validation_labels))\n",
    "    \n",
    "# Gaussian Naive Bayes\n",
    "nb_model_accuracies = []\n",
    "for fold_num in range(5):\n",
    "    train_data, validation_data = k_fold_splitter(fold_num, x)\n",
    "    train_labels, validation_labels = k_fold_splitter(fold_num, y)\n",
    "\n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(train_data, train_labels)\n",
    "\n",
    "    nb_model_accuracies.append(nb_model.score(validation_data, validation_labels))\n",
    "\n",
    "print(\"Softmax Regression Accuracy: {}\".format(softmax_accuracy))\n",
    "print(\"Ridge Regression (SKLearn) Accuracy: {}\".format(np.average(ridge_model_accuracies)))\n",
    "print(\"Logistic Regression (SKLearn) Accuracy: {}\".format(np.average(logistic_model_accuracies)))\n",
    "print(\"Gaussian Naive Bayes (SKLearn) Accuracy: {}\".format(np.average(nb_model_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
