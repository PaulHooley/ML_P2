{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent with Minibatch and Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MomentumGradientDescent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate=0.001,\n",
    "        momentum=0.9,\n",
    "        max_iters=1e4,\n",
    "        epsilon=1e-8,\n",
    "        batch_size=32,\n",
    "        record_history=False,\n",
    "    ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.max_iters = max_iters\n",
    "        self.record_history = record_history\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.prev_delta_w = None\n",
    "        if record_history:\n",
    "            # to store the weight history for visualization\n",
    "            self.w_history = []\n",
    "\n",
    "    def run(self, gradient_fn, x, y, w):\n",
    "        grad = np.inf\n",
    "        t = 1\n",
    "        N, D = x.shape\n",
    "        self.prev_delta_w = np.zeros(w.shape)\n",
    "        while np.linalg.norm(grad) > self.epsilon and t < self.max_iters:\n",
    "            for i in range(0, N, self.batch_size):\n",
    "                if x.ndim == 1:\n",
    "                    batch_x = x[i:i + self.batch_size]\n",
    "                else:\n",
    "                    batch_x = x[i:i + self.batch_size, :]\n",
    "\n",
    "                if y.ndim == 1:\n",
    "                    batch_y = y[i:i + self.batch_size]\n",
    "                else:\n",
    "                    batch_y = y[i:i + self.batch_size, :]\n",
    "\n",
    "                # compute the gradient with present weight\n",
    "                grad = gradient_fn(batch_x, batch_y, w)\n",
    "                delta_w = self.get_delta_w(grad)\n",
    "\n",
    "                # weight update step\n",
    "                w = w - self.learning_rate * delta_w\n",
    "                if self.record_history:\n",
    "                    self.w_history.append(w)\n",
    "            t += 1\n",
    "        return w\n",
    "\n",
    "    def get_delta_w(self, grad):\n",
    "        beta = self.momentum\n",
    "        delta_w = beta * self.prev_delta_w + (1 - beta) * grad\n",
    "        self.prev_delta_w = delta_w\n",
    "\n",
    "        return delta_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression\n",
    "\n",
    "Below is our implementation of the Softmax Regression model. Class labels and predictions are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# from the given Colab code\n",
    "logistic = lambda z: 1./ (1 + np.exp(-z))  \n",
    "\n",
    "class SoftmaxRegression:\n",
    "\n",
    "    def __init__(self, add_bias=True, regularization_penalty=0.):\n",
    "        self.add_bias = add_bias\n",
    "        self.regularization_penalty = regularization_penalty\n",
    "            \n",
    "    def fit(self, x, y, C, optimizer):\n",
    "        if x.ndim == 1:\n",
    "            x = x[:, None]\n",
    "        if self.add_bias:\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "        N,D = x.shape\n",
    "        \n",
    "        def to_onehot(a):\n",
    "            return np.eye(C)[a]\n",
    "        \n",
    "        def gradient(x, y, w):\n",
    "            N, D = x.shape\n",
    "            # yh: N x C\n",
    "            yh = self.softmax(np.dot(x, w))\n",
    "            # both are N x C\n",
    "            yh = to_onehot(self.to_classlabel(yh))\n",
    "            y = to_onehot(y)\n",
    "            \n",
    "            grad = np.dot(x.T, yh - y) / N\n",
    "            if self.regularization_penalty > 0:\n",
    "                if self.add_bias:\n",
    "                    grad[:-1,:] += self.regularization_penalty * w[:-1,:]    # don't penalize the intercept\n",
    "                else:\n",
    "                    grad += self.regularization_penalty * w\n",
    "            return grad\n",
    "        \n",
    "        # initialize all weights to random values\n",
    "        w0 = np.random.rand(D,C)  \n",
    "        # run the optimizer to get the optimal weights\n",
    "        self.w = optimizer.run(gradient, x, y, w0) \n",
    "        return self\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        # to prevent overflow/underflow\n",
    "        z = z - np.max(z, axis=-1, keepdims=True)\n",
    "        return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n",
    "\n",
    "    def to_classlabel(self, z):\n",
    "        return z.argmax(axis=1)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if self.add_bias:\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "        # convert from 1D to 2D\n",
    "        x = np.reshape(x, (1, -1))\n",
    "        yh = self.softmax(np.dot(x, self.w))\n",
    "        return self.to_classlabel(yh)[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of Model\n",
    "\n",
    "Throughout the code below, validation is performed using 5-fold cross-validation. Here we define some helper methods which are used in the following sections below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def k_fold_splitter(fold, dataset):\n",
    "    \"\"\"\n",
    "    Returns 2 datasets (training and validation)\n",
    "    \"\"\"\n",
    "    start = math.floor(fold*(dataset.shape[0]/5))\n",
    "    end = math.floor((fold+1)*(dataset.shape[0]/5))\n",
    "\n",
    "    training = np.delete(dataset, slice(start, end-1), axis=0)\n",
    "    validation = dataset[start:end-1]\n",
    "\n",
    "    return training, validation\n",
    "\n",
    "def calculate_model_accuracy(x, y, C, learning_rate, momentum, batch_size, regularization_penalty):\n",
    "    \"\"\"\n",
    "    Helper method to calculate the accuracy of our implemented Softmax\n",
    "    Regression model for a given set of inputs, labels, and hyper-parameters\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "    # do 5-fold cross-validation\n",
    "    for fold_num in range(5):\n",
    "        train_data, validation_data = k_fold_splitter(fold_num, x)\n",
    "        train_labels, validation_labels = k_fold_splitter(fold_num, y)\n",
    "\n",
    "        optimizer = MomentumGradientDescent(\n",
    "            learning_rate=learning_rate, \n",
    "            momentum=momentum, \n",
    "            batch_size=batch_size, \n",
    "            max_iters=10000,\n",
    "        )\n",
    "        model = SoftmaxRegression(add_bias=False, regularization_penalty=regularization_penalty)\n",
    "        model.fit(train_data, train_labels, C, optimizer)\n",
    "\n",
    "        num_misclassified = 0\n",
    "        # calculate the accuracy\n",
    "        for i in range(len(validation_data)):\n",
    "            prediction = model.predict(validation_data[i, :])\n",
    "            if prediction != validation_labels[i]:\n",
    "                num_misclassified += 1\n",
    "\n",
    "        misclassification_rate = num_misclassified / len(validation_labels)\n",
    "        accuracies.append(1 - misclassification_rate)\n",
    "        \n",
    "    return np.average(accuracies)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 Regularization\n",
    "\n",
    "## Digits Dataset\n",
    "\n",
    "To limit the runtime in this section, we use the best hyperparameters that we found in the SoftmaxRegression.ipynb notebook instead of recomputing them here. Note that this will still take a very long time to terminate for the digits dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "x, y = digits.data, digits.target\n",
    "\n",
    "# Parameters that were obtained from the grid serach in the SoftmaxRegression.ipynb notebook\n",
    "digits_learning_rate = 0.14003\n",
    "digits_momentum = 0.99\n",
    "digits_batch_size = 897\n",
    "\n",
    "C = 10\n",
    "max_accuracy = 0\n",
    "digits_penalty = 0\n",
    "\n",
    "for penalty in np.linspace(0, 1, 11):\n",
    "    accuracy = calculate_model_accuracy(x, y, C, digits_learning_rate, digits_momentum, digits_batch_size, penalty)\n",
    "    if accuracy > max_accuracy:\n",
    "        print(accuracy, end=', ')\n",
    "        max_accuracy = accuracy\n",
    "        digits_penalty = penalty\n",
    "            \n",
    "print(\"\\n\\nBest Accuracy: {}\".format(max_accuracy))  \n",
    "print(\"Best Regularization penalty: {}\".format(digits_penalty))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine Dataset\n",
    "\n",
    "Similar to above, we use the hyperparameters obtained in the other notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.906890756302521, \n",
      "\n",
      "Best Accuracy: 0.906890756302521\n",
      "Best Regularization penalty: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "x, y = wine.data, wine.target\n",
    "\n",
    "# Optimal parameters that were obtained from the grid search in the SoftmaxRegression.ipynb notebook\n",
    "wine_learning_rate = 0.14003\n",
    "wine_momentum = 0.696\n",
    "wine_batch_size = 178\n",
    "\n",
    "C = 10\n",
    "max_accuracy = 0\n",
    "wine_penalty = 0\n",
    "\n",
    "for penalty in np.linspace(0, 1, 11):\n",
    "    accuracy = calculate_model_accuracy(x, y, C, wine_learning_rate, wine_momentum, wine_batch_size, penalty)\n",
    "    if accuracy > max_accuracy:\n",
    "        print(accuracy, end=', ')\n",
    "        max_accuracy = accuracy\n",
    "        wine_penalty = penalty           \n",
    "            \n",
    "print(\"\\n\\nBest Accuracy: {}\".format(max_accuracy))\n",
    "print(\"Best Regularization penalty: {}\".format(wine_penalty))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Against Other Classifiers\n",
    "\n",
    "Now we can compare our model to an off-the-shelf classifier to see how the performance compares on the digits dataset. We will be comparing against the Ridge Regression and Logistic Regression models from SciKit Learn.\n",
    "\n",
    "### Digits Dataset Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Regression Accuracy: 0.9146076158167474\n",
      "Ridge Regression (SKLearn) Accuracy: 0.8889653133315697\n",
      "Logistic Regression (SKLearn) Accuracy: 0.918524454957128\n",
      "Multinomial Naive Bayes (SKLearn) Accuracy: 0.8716826691150154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "digits = load_digits()\n",
    "x, y = digits.data, digits.target\n",
    "\n",
    "C = 10\n",
    "# using the hyper-parameters obtained from grid search\n",
    "softmax_accuracy = calculate_model_accuracy(x, y, C, digits_learning_rate, digits_momentum, digits_batch_size, 0)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_model_accuracies = []\n",
    "for fold_num in range(5):\n",
    "    train_data, validation_data = k_fold_splitter(fold_num, x)\n",
    "    train_labels, validation_labels = k_fold_splitter(fold_num, y)\n",
    "\n",
    "    ridge_model = RidgeClassifier(max_iter=10000)\n",
    "    ridge_model.fit(train_data, train_labels)\n",
    "\n",
    "    ridge_model_accuracies.append(ridge_model.score(validation_data, validation_labels))\n",
    "    \n",
    "# Logistic Regression\n",
    "logistic_model_accuracies = []\n",
    "for fold_num in range(5):\n",
    "    train_data, validation_data = k_fold_splitter(fold_num, x)\n",
    "    train_labels, validation_labels = k_fold_splitter(fold_num, y)\n",
    "\n",
    "    logistic_model = LogisticRegression(multi_class=\"multinomial\", max_iter=10000)\n",
    "    logistic_model.fit(train_data, train_labels)\n",
    "\n",
    "    logistic_model_accuracies.append(logistic_model.score(validation_data, validation_labels))\n",
    "    \n",
    "# Multinomial Naive Bayes\n",
    "nb_model_accuracies = []\n",
    "for fold_num in range(5):\n",
    "    train_data, validation_data = k_fold_splitter(fold_num, x)\n",
    "    train_labels, validation_labels = k_fold_splitter(fold_num, y)\n",
    "\n",
    "    nb_model = MultinomialNB()\n",
    "    nb_model.fit(train_data, train_labels)\n",
    "\n",
    "    nb_model_accuracies.append(nb_model.score(validation_data, validation_labels))\n",
    "\n",
    "print(\"Softmax Regression Accuracy: {}\".format(softmax_accuracy))\n",
    "print(\"Ridge Regression (SKLearn) Accuracy: {}\".format(np.average(ridge_model_accuracies)))\n",
    "print(\"Logistic Regression (SKLearn) Accuracy: {}\".format(np.average(logistic_model_accuracies)))\n",
    "print(\"Multinomial Naive Bayes (SKLearn) Accuracy: {}\".format(np.average(nb_model_accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine Dataset Comparison\n",
    "\n",
    "We can do the same comparison on the wine dataset. This is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Regression Accuracy: 0.8667226890756303\n",
      "Ridge Regression (SKLearn) Accuracy: 0.9421848739495797\n",
      "Logistic Regression (SKLearn) Accuracy: 0.9359663865546219\n",
      "Gaussian Naive Bayes (SKLearn) Accuracy: 0.9594957983193277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "wine = load_wine()\n",
    "x, y = wine.data, wine.target\n",
    "\n",
    "C = 3\n",
    "# using the hyper-parameters obtained from grid search\n",
    "softmax_accuracy = calculate_model_accuracy(x, y, C, wine_learning_rate, wine_momentum, wine_batch_size, wine_penalty)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge_model_accuracies = []\n",
    "for fold_num in range(5):\n",
    "    train_data, validation_data = k_fold_splitter(fold_num, x)\n",
    "    train_labels, validation_labels = k_fold_splitter(fold_num, y)\n",
    "\n",
    "    ridge_model = RidgeClassifier(max_iter=10000)\n",
    "    ridge_model.fit(train_data, train_labels)\n",
    "\n",
    "    ridge_model_accuracies.append(ridge_model.score(validation_data, validation_labels))\n",
    "    \n",
    "# Logistic Regression\n",
    "logistic_model_accuracies = []\n",
    "for fold_num in range(5):\n",
    "    train_data, validation_data = k_fold_splitter(fold_num, x)\n",
    "    train_labels, validation_labels = k_fold_splitter(fold_num, y)\n",
    "\n",
    "    logistic_model = LogisticRegression(multi_class=\"multinomial\", max_iter=10000)\n",
    "    logistic_model.fit(train_data, train_labels)\n",
    "\n",
    "    logistic_model_accuracies.append(logistic_model.score(validation_data, validation_labels))\n",
    "    \n",
    "# Gaussian Naive Bayes\n",
    "nb_model_accuracies = []\n",
    "for fold_num in range(5):\n",
    "    train_data, validation_data = k_fold_splitter(fold_num, x)\n",
    "    train_labels, validation_labels = k_fold_splitter(fold_num, y)\n",
    "\n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(train_data, train_labels)\n",
    "\n",
    "    nb_model_accuracies.append(nb_model.score(validation_data, validation_labels))\n",
    "\n",
    "print(\"Softmax Regression Accuracy: {}\".format(softmax_accuracy))\n",
    "print(\"Ridge Regression (SKLearn) Accuracy: {}\".format(np.average(ridge_model_accuracies)))\n",
    "print(\"Logistic Regression (SKLearn) Accuracy: {}\".format(np.average(logistic_model_accuracies)))\n",
    "print(\"Gaussian Naive Bayes (SKLearn) Accuracy: {}\".format(np.average(nb_model_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
